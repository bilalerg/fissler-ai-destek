import os
import glob
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

# --- AYARLAR ---
DATA_PATH = "belgeler"  # Senin klasÃ¶rÃ¼n burasÄ±
DB_FAISS_PATH = "faiss_index"


def determine_family(filename):
    """Dosya ismine bakarak Ã¼rÃ¼n ailesini belirler."""
    name = filename.lower()

    if "vitaquick" in name:
        return "vitaquick"
    elif "vitavit" in name:
        return "vitavit"
    elif "adamant" in name:
        return "adamant"
    else:
        # Garanti belgesi veya genel kÄ±lavuzlar buraya dÃ¼ÅŸer
        return "genel"


def create_vector_db():
    if not os.path.exists(DATA_PATH):
        print(f"âŒ '{DATA_PATH}' klasÃ¶rÃ¼ bulunamadÄ±! LÃ¼tfen klasÃ¶r ismini kontrol et.")
        return

    print(f"ğŸ“‚ '{DATA_PATH}' klasÃ¶rÃ¼ndeki PDF'ler taranÄ±yor...")

    all_documents = []

    # KlasÃ¶rdeki tÃ¼m PDF'leri bul
    pdf_files = glob.glob(os.path.join(DATA_PATH, "*.pdf"))

    if not pdf_files:
        print("âŒ KlasÃ¶rde hiÃ§ PDF dosyasÄ± yok.")
        return

    for pdf_path in pdf_files:
        filename = os.path.basename(pdf_path)
        family = determine_family(filename)

        print(f"   ğŸ‘‰ Okunuyor: {filename} [Etiket: {family}]")

        try:
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()

            # ğŸ“Œ YENÄ°: Her sayfaya metadata (etiket) ekle + kaynak dosya adÄ±
            for doc in docs:
                doc.metadata["family"] = family
                doc.metadata["source"] = filename
                doc.metadata["source_file"] = filename  # Ekstra alan (daha net filtreleme iÃ§in)

            all_documents.extend(docs)
            print(f"      âœ… {len(docs)} sayfa yÃ¼klendi")
        except Exception as e:
            print(f"   âš ï¸ HATA: {filename} okunamadÄ±. Sebebi: {e}")

    print(f"\nâœ… Toplam {len(all_documents)} sayfa yÃ¼klendi. ParÃ§alanÄ±yor...")

    # Chunk ayarlarÄ±nÄ± geniÅŸ tutuyoruz (Daha Ã¶nceki baÅŸarÄ±mÄ±zdan dolayÄ±)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    texts = text_splitter.split_documents(all_documents)
    print(f"ğŸ’¾ {len(texts)} parÃ§a (chunk) oluÅŸturuldu. VeritabanÄ±na iÅŸleniyor...")

    # ğŸ“Œ DEBUG: Ä°lk chunk'Ä±n metadata'sÄ±nÄ± gÃ¶relim
    if texts:
        print(f"\nğŸ” Ã–RNEK CHUNK METADATA:")
        print(f"   Family: {texts[0].metadata.get('family')}")
        print(f"   Source: {texts[0].metadata.get('source')}")
        print(f"   Source File: {texts[0].metadata.get('source_file')}")
        print(f"   Ä°Ã§erik Ã–nizleme: {texts[0].page_content[:100]}...\n")

    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(texts, embeddings)
    db.save_local(DB_FAISS_PATH)

    print(f"ğŸ‰ FAISS veritabanÄ± baÅŸarÄ±yla gÃ¼ncellendi! AyrÄ±m yapÄ±ldÄ± (Vitaquick/Vitavit/Adamant).")
    print(f"ğŸ“Š Ä°statistikler:")
    print(f"   - Toplam PDF: {len(pdf_files)}")
    print(f"   - Toplam Sayfa: {len(all_documents)}")
    print(f"   - Toplam Chunk: {len(texts)}")

    # Aile bazÄ±nda sayÄ±larÄ± gÃ¶ster
    family_counts = {}
    for doc in all_documents:
        family = doc.metadata.get("family", "bilinmiyor")
        family_counts[family] = family_counts.get(family, 0) + 1

    print(f"\nğŸ“ Aile BazÄ±nda DaÄŸÄ±lÄ±m:")
    for family, count in sorted(family_counts.items()):
        print(f"   - {family.upper()}: {count} sayfa")


if __name__ == "__main__":
    create_vector_db()